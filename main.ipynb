{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oNOGaqvwhmE",
        "outputId": "468ef08b-8b44-42d0-a1cc-90bb491f95b0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import xml.etree.ElementTree as ET\n",
        "from datetime import datetime\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# For clustering speaker embeddings\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# For evaluation metrics (pyannote)\n",
        "from pyannote.metrics.diarization import DiarizationErrorRate\n",
        "from pydub import AudioSegment\n",
        "# For ASR using OpenAI Whisper\n",
        "import whisper\n",
        "\n",
        "# For speaker embeddings via SpeechBrain\n",
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "import soundfile as sf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "eHljLF1NwkAH"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Global Constants and Paths\n",
        "# -------------------------------\n",
        "AMI_AUDIO_PATH = \"./AMI/ES2008a.wav\"  # Mixed audio file for session ES2008a\n",
        "AMI_ANNOTATIONS_DIR = \"./AMI/ES2008a/\"  # AMI annotation XML file\n",
        "OUTPUT_CSV_PATH = \"./Outputs/ES2008a_transcript.csv\"  # Output CSV file for ASR results\n",
        "OUTPUT_SPEAKER_EMBEDDINGS_DIR = \"./Outputs/ES2008a_embeddings/\"  # Directory for speaker embeddings\n",
        "OUTPUT_SPEAKER_EMBEDDINGS_CSV = \"./Outputs/ES2008a_embeddings.csv\"  # CSV for speaker embeddings\n",
        "ENROLLED_TEMPLATES_DIR = \"./voxceleb2/\"\n",
        "CUSTOM_AUDIO_PATH = \"./custom_hinglish_audio.wav\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "TARGET_SR = 16000\n",
        "DISTANCE_THRESHOLD = 0.015\n",
        "VAD_THRESHOLD = 0.4\n",
        "MIN_SPEECH_DURATION = 1.04\n",
        "RECOGNITION_THRESHOLD = 0.45\n",
        "GAP_THRESHOLD = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE4qk_u10324",
        "outputId": "5e2d8322-f8dd-4412-c083-e880205e91fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEVICE mps\n"
          ]
        }
      ],
      "source": [
        "print(\"DEVICE\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "4sq94GKW2-1c"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Load Audio File\n",
        "# -------------------------------\n",
        "def load_audio(file_path, target_sr=TARGET_SR):\n",
        "    \"\"\"\n",
        "    Load an audio file using torchaudio and resample to target sampling rate if necessary.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the audio file.\n",
        "        target_sr (int): Target sampling rate (Hz).\n",
        "\n",
        "    Returns:\n",
        "        waveform (Tensor): Audio waveform (1, N) as torch.float32.\n",
        "        sr (int): Sampling rate.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        file_path = os.path.expanduser(file_path)\n",
        "        # Check if file exists\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "        \n",
        "        if file_path.lower().endswith((\".m4a\", \".mp4\")):\n",
        "            # try:\n",
        "            # waveform, sr = torchaudio.load(file_path,format=\"mp4\")\n",
        "            audio = AudioSegment.from_file(file_path, format=\"m4a\")\n",
        "            # Convert AudioSegment to numpy array\n",
        "            samples = np.array(audio.get_array_of_samples())\n",
        "            waveform = torch.from_numpy(samples.astype(np.float32) / 32768.0).unsqueeze(0)\n",
        "            sr = audio.frame_rate\n",
        "            # except Exception as e:\n",
        "                # print(f\"torchaudio.load failed for {file_path} with format='mp4': {e}\")\n",
        "                # Fallback: use pydub to load the file.\n",
        "        else:\n",
        "            import soundfile as sf\n",
        "            # Set the audio backend to sox_io (recommended on macOS)\n",
        "            data, sr = sf.read(file_path)\n",
        "            if len(data.shape) > 1:\n",
        "                data = data[:, 0]  # use first channel if multi-channel\n",
        "            waveform = torch.from_numpy(data).unsqueeze(0).float()\n",
        "        \n",
        "        # Resample if needed\n",
        "        if sr != target_sr:\n",
        "            resampler = torchaudio.transforms.Resample(sr, target_sr)\n",
        "            waveform = resampler(waveform)\n",
        "            sr = target_sr\n",
        "        return waveform, sr\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading audio file {file_path}: {e}\")\n",
        "        sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "mp7sRoHS3BWW"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Perform Voice Activity Detection (VAD)\n",
        "# -------------------------------\n",
        "def perform_vad(waveform, sr, vad_threshold=VAD_THRESHOLD, min_speech_duration=MIN_SPEECH_DURATION):\n",
        "    \"\"\"\n",
        "    Apply Silero VAD to detect and extract speech segments from the audio.\n",
        "\n",
        "    Args:\n",
        "        waveform (Tensor): Input audio waveform (1, N).\n",
        "        sr (int): Sampling rate.\n",
        "        vad_threshold (float): Probability threshold for speech.\n",
        "        min_speech_duration (float): Minimum speech segment duration (in seconds).\n",
        "\n",
        "    Returns:\n",
        "        speech_segments (list of dict): Each dict has 'start' and 'end' (in seconds).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Necessary for using a forked repo in Colab\n",
        "        torch.hub._validate_not_a_forked_repo = lambda repo_owner, repo_name, ref: True\n",
        "        # Load Silero VAD model and utilities\n",
        "        models, utils = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", trust_repo=True)\n",
        "        (get_speech_ts, _, _, _, _) = utils\n",
        "\n",
        "        # Run VAD to get speech timestamps\n",
        "        speech_timestamps = get_speech_ts(waveform, model=models, sampling_rate=sr, threshold=vad_threshold)\n",
        "\n",
        "        # Determine if the returned timestamps appear to be in samples rather than seconds.\n",
        "        audio_duration_sec = waveform.shape[1] / sr  # total duration in seconds\n",
        "\n",
        "        # Filter out very short segments\n",
        "        speech_segments = []\n",
        "        if any(seg[\"start\"] > audio_duration_sec for seg in speech_timestamps):\n",
        "            # Timestamps seem to be in samples; convert to seconds.\n",
        "            print('Timestamps seem to be in samples; converting to seconds.')\n",
        "            for seg in speech_timestamps:\n",
        "                # print(seg)\n",
        "                # print(\"Segment Start\",seg[\"start\"])\n",
        "                # print(\"Segment End\",seg[\"end\"])\n",
        "                seg[\"start\"] = seg[\"start\"] / sr\n",
        "                seg[\"end\"]   = seg[\"end\"]   / sr\n",
        "                duration = seg[\"end\"] - seg[\"start\"]\n",
        "                # print(\"Duration\",duration)\n",
        "                if duration >= min_speech_duration:\n",
        "                    speech_segments.append({\"start\": seg[\"start\"], \"end\": seg[\"end\"]})\n",
        "        else:\n",
        "            # Timestamps are already in seconds.\n",
        "            print('Timestamps are already in seconds.')\n",
        "            for seg in speech_timestamps:\n",
        "                duration = seg[\"end\"] - seg[\"start\"]\n",
        "                # print(\"Duration\",duration)\n",
        "                if duration >= min_speech_duration:\n",
        "                    speech_segments.append({\"start\": seg[\"start\"], \"end\": seg[\"end\"]})\n",
        "        return speech_segments\n",
        "    except Exception as e:\n",
        "        print(f\"Error during VAD: {e}\")\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "-wVIY6IF3DT0"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Extract Audio Segments from VAD Output\n",
        "# -------------------------------\n",
        "def extract_audio_segments(waveform, sr, segments):\n",
        "    \"\"\"\n",
        "    Extract waveform segments corresponding to the speech timestamps.\n",
        "\n",
        "    Args:\n",
        "        waveform (Tensor): Full audio waveform (1, N).\n",
        "        sr (int): Sampling rate.\n",
        "        segments (list of dict): List with 'start' and 'end' in seconds.\n",
        "\n",
        "    Returns:\n",
        "        list: List of Tensors corresponding to each speech segment.\n",
        "    \"\"\"\n",
        "    segment_list = []\n",
        "    for seg in segments:\n",
        "        start_sample = int(seg[\"start\"] * sr)\n",
        "        end_sample = int(seg[\"end\"] * sr)\n",
        "        segment_list.append(waveform[:, start_sample:end_sample])\n",
        "    return segment_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "4BMCnnDP3F6W"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Extract Speaker Embeddings for Each Segment\n",
        "# -------------------------------\n",
        "def extract_embeddings(segments_audio, spk_model, sr=TARGET_SR):\n",
        "    \"\"\"\n",
        "    Extract speaker embeddings for each speech segment.\n",
        "    Pads segments shorter than 1 second.\n",
        "    \"\"\"\n",
        "    embeddings = []\n",
        "    min_samples = int(1.0 * sr)  # minimum 1 second length\n",
        "    for seg in segments_audio:\n",
        "        try:\n",
        "            # Pad segment if too short\n",
        "            if seg.shape[1] < min_samples:\n",
        "                pad_amount = min_samples - seg.shape[1]\n",
        "                seg = F.pad(seg, (0, pad_amount))\n",
        "            seg = seg.to(DEVICE)\n",
        "            with torch.no_grad():\n",
        "                emb = spk_model.encode_batch(seg)\n",
        "            embeddings.append(emb.squeeze().cpu().numpy())\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting embedding for a segment: {e}\")\n",
        "    if not embeddings:\n",
        "        print(\"No embeddings extracted!\")\n",
        "        sys.exit(1)\n",
        "    return np.vstack(embeddings)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "Ld4feBCL3HhF"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Cluster Speaker Embeddings (Diarization)\n",
        "# -------------------------------\n",
        "def cluster_embeddings(embeddings, distance_threshold=DISTANCE_THRESHOLD):\n",
        "    \"\"\"\n",
        "    Cluster speaker embeddings using Agglomerative Clustering with cosine distance.\n",
        "\n",
        "    Args:\n",
        "        embeddings (np.ndarray): Array of speaker embeddings (n_segments, embedding_dim).\n",
        "        distance_threshold (float): Threshold to decide cluster merging.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of cluster labels for each segment.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Using AgglomerativeClustering with distance_threshold requires n_clusters=None.\n",
        "        clusterer = AgglomerativeClustering(\n",
        "    n_clusters=None, metric=\"cosine\", linkage=\"average\", distance_threshold=distance_threshold\n",
        ")\n",
        "        labels = clusterer.fit_predict(embeddings)\n",
        "        return labels\n",
        "    except Exception as e:\n",
        "        print(f\"Error during clustering: {e}\")\n",
        "        sys.exit(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "L7hTuSb73KFh"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Assign Generic Speaker Labels\n",
        "# -------------------------------\n",
        "def assign_speaker_labels(cluster_labels):\n",
        "    \"\"\"\n",
        "    Assign generic speaker names (e.g., Speaker 1, Speaker 2, ...) based on cluster labels.\n",
        "\n",
        "    Args:\n",
        "        cluster_labels (np.ndarray): Array of integer cluster labels.\n",
        "\n",
        "    Returns:\n",
        "        list: List of speaker label strings corresponding to each segment.\n",
        "    \"\"\"\n",
        "    unique_labels = sorted(set(cluster_labels))\n",
        "    label_map = {label: f\"Speaker {i+1}\" for i, label in enumerate(unique_labels)}\n",
        "    speaker_labels = [label_map[label] for label in cluster_labels]\n",
        "    return speaker_labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_voxceleb2_files(root_dir):\n",
        "    \"\"\"\n",
        "    Recursively traverse the VoxCeleb2 AAC directory to collect all .m4a file paths \n",
        "    and map them to speaker IDs. This function assumes that the directory structure is:\n",
        "        <root_dir>/aac/<speaker_id>/<subfolder>/.../<filename>.m4a\n",
        "    and that the speaker ID is the folder immediately following the \"aac\" folder.\n",
        "    \n",
        "    Args:\n",
        "        root_dir (str): Root directory of the VoxCeleb2 data.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Mapping from speaker_id (str) to a list of file paths (str).\n",
        "    \"\"\"\n",
        "    templates_paths = {}\n",
        "    aac_path = os.path.join(root_dir, \"aac\")\n",
        "    if not os.path.exists(aac_path):\n",
        "        print(f\"AAC folder not found in VoxCeleb2 root directory: {root_dir}\")\n",
        "        return templates_paths\n",
        "    # Walk through the 'aac' folder recursively\n",
        "    for dirpath, _, filenames in os.walk(aac_path):\n",
        "        for filename in filenames:\n",
        "            if filename.lower().endswith(\".m4a\"):\n",
        "                full_path = os.path.join(dirpath, filename)\n",
        "                # Split the path and extract the speaker_id as the folder immediately after \"aac\"\n",
        "                parts = os.path.normpath(full_path).split(os.sep)\n",
        "                try:\n",
        "                    idx = parts.index(\"aac\")\n",
        "                    # The speaker_id is assumed to be the next folder after \"aac\"\n",
        "                    speaker_id = parts[idx + 1]\n",
        "                except (ValueError, IndexError):\n",
        "                    speaker_id = \"Unknown\"\n",
        "                if speaker_id not in templates_paths:\n",
        "                    templates_paths[speaker_id] = []\n",
        "                templates_paths[speaker_id].append(full_path)\n",
        "    return templates_paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_voxceleb2_templates(root_dir, spk_model, target_sr=TARGET_SR):\n",
        "    \"\"\"\n",
        "    Load enrolled speaker templates from VoxCeleb2 data.\n",
        "    For each speaker, it uses the first available audio file (organized under the \"aac\" folder)\n",
        "    to compute a reference embedding using the pretrained speaker embedding model.\n",
        "    \n",
        "    Args:\n",
        "        root_dir (str): Root directory of VoxCeleb2 (should contain the \"aac\" subfolder).\n",
        "        spk_model: Pretrained SpeechBrain speaker embedding model.\n",
        "        target_sr (int): Desired sampling rate.\n",
        "    \n",
        "    Returns:\n",
        "        dict: Mapping from speaker_id to enrollment embedding (numpy array).\n",
        "    \"\"\"\n",
        "    enrolled_templates = {}\n",
        "    templates_paths = collect_voxceleb2_files(root_dir)\n",
        "    for speaker_id, file_list in templates_paths.items():\n",
        "        if not file_list:\n",
        "            continue\n",
        "        embeddings_list = []\n",
        "        # For each speaker, use the first audio file in the list for enrollment.\n",
        "        for template_path in file_list[:10]:\n",
        "            try:\n",
        "                waveform, sr = load_audio(template_path, target_sr=target_sr)\n",
        "                # If the audio is too long, extract a centered two-second chunk.\n",
        "                if waveform.shape[1] > target_sr * 2:\n",
        "                    mid = waveform.shape[1] // 2\n",
        "                    waveform = waveform[:, mid - target_sr: mid + target_sr]\n",
        "                with torch.no_grad():\n",
        "                    emb = spk_model.encode_batch(waveform.to(DEVICE)).squeeze().cpu().numpy()\n",
        "                enrolled_templates[speaker_id] = emb\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing VoxCeleb2 file {template_path}: {e}\")\n",
        "        if embeddings_list:\n",
        "            # Compute the average embedding for a more robust enrollment representation.\n",
        "            enrolled_templates[speaker_id] = np.mean(embeddings_list, axis=0)\n",
        "    return enrolled_templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: load_enrolled_templates for speaker recognition\n",
        "# -------------------------------\n",
        "def load_enrolled_templates(enrolled_dir, spk_model, target_sr=TARGET_SR):\n",
        "    \"\"\"\n",
        "    Load enrolled speaker audio files and extract embeddings.\n",
        "    \n",
        "    Args:\n",
        "        enrolled_dir (str): Directory containing enrolled WAV files.\n",
        "        spk_model: Pretrained speaker encoder.\n",
        "        target_sr (int): Sampling rate.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping file base names to embedding vectors.\n",
        "    \"\"\"\n",
        "    enrolled_templates = {}\n",
        "    audio_files = glob.glob(os.path.join(enrolled_dir, \"*.wav\"))\n",
        "    if not audio_files:\n",
        "        print(\"No enrolled templates found. Speaker recognition will use generic labels.\")\n",
        "        return enrolled_templates\n",
        "    for file in audio_files:\n",
        "        try:\n",
        "            waveform, sr = load_audio(file, target_sr=target_sr)\n",
        "            # Use a central 2-second chunk if audio is too long\n",
        "            if waveform.shape[1] > sr * 2:\n",
        "                mid = waveform.shape[1] // 2\n",
        "                waveform = waveform[:, mid - sr: mid + sr]\n",
        "            with torch.no_grad():\n",
        "                emb = spk_model.encode_batch(waveform.to(DEVICE)).squeeze().cpu().numpy()\n",
        "            key = os.path.splitext(os.path.basename(file))[0]\n",
        "            enrolled_templates[key] = emb\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing enrolled template {file}: {e}\")\n",
        "    return enrolled_templates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: perform_speaker_recognition\n",
        "# -------------------------------\n",
        "def perform_speaker_recognition(embeddings, cluster_labels, enrolled_templates, recognition_threshold=RECOGNITION_THRESHOLD):\n",
        "    \"\"\"\n",
        "    For each cluster, compute the mean embedding and compare to enrolled templates using cosine similarity.\n",
        "    \n",
        "    Args:\n",
        "        embeddings (np.ndarray): All speaker embeddings.\n",
        "        cluster_labels (np.ndarray): Cluster labels.\n",
        "        enrolled_templates (dict): Mapping of enrolled identity to embedding.\n",
        "        recognition_threshold (float): Cosine similarity threshold.\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping each cluster label to a recognized identity.\n",
        "    \"\"\"\n",
        "    recognized_ids = {}\n",
        "    if not enrolled_templates:\n",
        "        for label in sorted(set(cluster_labels)):\n",
        "            recognized_ids[label] = f\"Speaker {label+1}\"\n",
        "        return recognized_ids\n",
        "\n",
        "    unique_labels = sorted(set(cluster_labels))\n",
        "    for label in unique_labels:\n",
        "        # Compute the mean embedding for the current cluster\n",
        "        idx = np.where(cluster_labels == label)[0]\n",
        "        cluster_embs = embeddings[idx]\n",
        "        mean_emb = np.mean(cluster_embs, axis=0)\n",
        "\n",
        "        # Normalize the mean embedding to unit vector\n",
        "        norm_mean_emb = mean_emb / (np.linalg.norm(mean_emb) + 1e-8)\n",
        "\n",
        "        best_score = -1.0\n",
        "        best_id = None\n",
        "        for identity, temp_emb in enrolled_templates.items():\n",
        "            norm_temp_emb = temp_emb / (np.linalg.norm(temp_emb) + 1e-8)\n",
        "            cos_sim = np.dot(norm_mean_emb, norm_temp_emb)\n",
        "            if cos_sim > best_score:\n",
        "                best_score = cos_sim\n",
        "                best_id = identity\n",
        "        \n",
        "        if best_score >= recognition_threshold:\n",
        "            recognized_ids[label] = best_id\n",
        "        else:\n",
        "            print(f\"Low similarity score ({best_score:.3f}) for cluster {label}, assigning generic label.\")\n",
        "            recognized_ids[label] = f\"Speaker {label+1}\"\n",
        "    return recognized_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "P0dJMAkz3L7S"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Transcribe Speech Segments Using Whisper ASR\n",
        "# -------------------------------\n",
        "def transcribe_segments(waveform, sr, segments, speaker_labels, asr_model:whisper.Whisper, language=None):\n",
        "    \"\"\"\n",
        "    Transcribe each speech segment with Whisper ASR.\n",
        "\n",
        "    Args:\n",
        "        waveform (Tensor): Full audio waveform (1, N).\n",
        "        sr (int): Sampling rate.\n",
        "        segments (list of dict): List of speech segments with 'start' and 'end' times.\n",
        "        speaker_labels (list): List of speaker label strings for each segment.\n",
        "        asr_model: Loaded Whisper model.\n",
        "        language (str): Optional language parameter for ASR.\n",
        "\n",
        "    Returns:\n",
        "        list: List of tuples (start_time, end_time, speaker, transcript).\n",
        "    \"\"\"\n",
        "    transcript_list = []\n",
        "    for seg, spk in zip(segments, speaker_labels):\n",
        "        try:\n",
        "            start_sample = int(seg[\"start\"] * sr)\n",
        "            end_sample = int(seg[\"end\"] * sr)\n",
        "            # Extract segment and convert to numpy array in float32, normalized between -1 and 1.\n",
        "            seg_wave = waveform[:, start_sample:end_sample]\n",
        "            seg_audio = seg_wave.squeeze().cpu().numpy().astype(np.float32)\n",
        "            # Whisper expects mono audio at 16 kHz.\n",
        "            # Transcribe the segment (the model auto-detects language if not provided)\n",
        "            result = asr_model.transcribe(seg_audio, fp16=torch.cuda.is_available(), language=language)\n",
        "            transcript = result[\"text\"].strip()\n",
        "            transcript_list.append((seg[\"start\"], seg[\"end\"], spk, transcript))\n",
        "        except Exception as e:\n",
        "            print(f\"Error during transcription of segment {seg}: {e}\")\n",
        "            transcript_list.append((seg[\"start\"], seg[\"end\"], spk, \"\"))\n",
        "    return transcript_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: merge_segments\n",
        "# -------------------------------\n",
        "def merge_segments(segments, gap_threshold=GAP_THRESHOLD):\n",
        "    \"\"\"\n",
        "    Merge adjacent segments with the same speaker if the gap between segments is less than gap_threshold seconds.\n",
        "    \n",
        "    Args:\n",
        "        segments (list of dict): Each dict contains keys \"start\", \"end\", and optionally \"transcript\".\n",
        "        gap_threshold (float): Maximum gap (in seconds) to consider merging.\n",
        "    \n",
        "    Returns:\n",
        "        List of merged segments.\n",
        "    \"\"\"\n",
        "    if not segments:\n",
        "        return []\n",
        "    # Sort segments by start time\n",
        "    segments = sorted(segments, key=lambda x: x[\"start\"])\n",
        "    merged = [segments[0].copy()]\n",
        "    for seg in segments[1:]:\n",
        "        last_seg = merged[-1]\n",
        "        # Check if the same speaker and if gap is within threshold\n",
        "        if seg.get(\"speaker\") == last_seg.get(\"speaker\") and (seg[\"start\"] - last_seg[\"end\"]) <= gap_threshold:\n",
        "            # Merge segments: update end time and concatenate transcripts if available\n",
        "            last_seg[\"end\"] = seg[\"end\"]\n",
        "            if \"transcript\" in last_seg and last_seg[\"transcript\"] and seg.get(\"transcript\"):\n",
        "                last_seg[\"transcript\"] = last_seg[\"transcript\"].strip() + \" \" + seg[\"transcript\"].strip()\n",
        "            elif \"transcript\" in seg:\n",
        "                last_seg[\"transcript\"] = seg[\"transcript\"]\n",
        "        else:\n",
        "            merged.append(seg.copy())\n",
        "    return merged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "JhaqQGwM3Np8"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Save Transcript to CSV\n",
        "# -------------------------------\n",
        "def save_transcript_csv(transcript_list, output_csv_path):\n",
        "    \"\"\"\n",
        "    Save the final transcript to a CSV file.\n",
        "    \n",
        "    Args:\n",
        "        transcript_list (list): List of tuples (start_time, end_time, speaker, transcript).\n",
        "        output_csv_path (str): Output file path for the CSV.\n",
        "    \"\"\"\n",
        "    # Ensure the output directory exists\n",
        "    output_dir = os.path.dirname(output_csv_path)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "    df = pd.DataFrame(transcript_list, columns=[\"start_time\", \"end_time\", \"speaker\", \"transcript\"])\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"Transcript saved to {output_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "Y8fAJMJH3PIJ"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Parse a Single AMI Annotation XML File for Ground Truth\n",
        "# -------------------------------\n",
        "def parse_ami_annotation(xml_path):\n",
        "    \"\"\"\n",
        "    Parse a single AMI annotation XML file to extract ground truth speech segments.\n",
        "\n",
        "    Args:\n",
        "        xml_path (str): Path to the annotation XML file.\n",
        "\n",
        "    Returns:\n",
        "        list: List of dict objects, each with 'start' and 'end' keys (floats in seconds)\n",
        "              and optionally 'speaker' if available.\n",
        "    \"\"\"\n",
        "    gt_segments = []\n",
        "    try:\n",
        "        tree = ET.parse(xml_path)\n",
        "        root = tree.getroot()\n",
        "        for segment in root.iter('segment'):\n",
        "            # print(segment.attrib)\n",
        "            start = float(segment.attrib.get(\"transcriber_start\", \"0\"))\n",
        "            end = float(segment.attrib.get(\"transcriber_end\", \"0\"))\n",
        "            speaker = segment.attrib.get(\"{http://nite.sourceforge.net/}id\", \"Unknown\")\n",
        "            gt_segments.append({\"start\": start, \"end\": end, \"speaker\": speaker})\n",
        "        return gt_segments\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing AMI XML annotation {xml_path}: {e}\")\n",
        "        return []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "EIDTxGO43QqG"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Parse All AMI Annotation XML Files from a Directory\n",
        "# -------------------------------\n",
        "def parse_all_ami_annotations(annotations_dir):\n",
        "    \"\"\"\n",
        "    Parse all AMI annotation XML files in a directory and aggregate ground truth segments.\n",
        "\n",
        "    Args:\n",
        "        annotations_dir (str): Directory path containing AMI annotation XML files.\n",
        "\n",
        "    Returns:\n",
        "        list: Combined list of ground truth segments from all files.\n",
        "    \"\"\"\n",
        "    all_segments = []\n",
        "    xml_files = glob.glob(os.path.join(annotations_dir, \"*.segments.xml\"))\n",
        "    if not xml_files:\n",
        "        print(f\"No XML annotation files found in {annotations_dir}\")\n",
        "        return all_segments\n",
        "    for xml_file in sorted(xml_files):\n",
        "        segments = parse_ami_annotation(xml_file)\n",
        "        all_segments.extend(segments)\n",
        "    return all_segments\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "uPlvlizP3SNu"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Compute VAD F1-Score Using pyannote.metrics\n",
        "# -------------------------------\n",
        "def compute_vad_f1(detected_segments, reference_segments, total_duration):\n",
        "    \"\"\"\n",
        "    Compute VAD F1 score by comparing detected speech segments with reference ground truth.\n",
        "\n",
        "    Args:\n",
        "        detected_segments (list of dict): Speech segments from VAD with 'start' and 'end'.\n",
        "        reference_segments (list of dict): Ground truth speech segments.\n",
        "        total_duration (float): Total duration of the audio in seconds.\n",
        "        tolerance (float): Forgiveness collar in seconds.\n",
        "\n",
        "    Returns:\n",
        "        float: F1 score.\n",
        "    \"\"\"\n",
        "    # Create binary time series for reference and hypothesis\n",
        "    # Discretize time with resolution e.g., 0.1 sec\n",
        "    resolution = 0.1\n",
        "    times = np.arange(0, total_duration, resolution)\n",
        "    ref = np.zeros_like(times)\n",
        "    hyp = np.zeros_like(times)\n",
        "\n",
        "    for seg in reference_segments:\n",
        "        start_idx = int(seg[\"start\"] / resolution)\n",
        "        end_idx = int(seg[\"end\"] / resolution)\n",
        "        ref[start_idx:end_idx] = 1\n",
        "    for seg in detected_segments:\n",
        "        start_idx = int(seg[\"start\"] / resolution)\n",
        "        end_idx = int(seg[\"end\"] / resolution)\n",
        "        hyp[start_idx:end_idx] = 1\n",
        "\n",
        "    # Compute true positives, false positives, false negatives\n",
        "    tp = np.sum((ref == 1) & (hyp == 1))\n",
        "    fp = np.sum((ref == 0) & (hyp == 1))\n",
        "    fn = np.sum((ref == 1) & (hyp == 0))\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
        "\n",
        "    return f1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "w4OfjR0YwekR"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Function: Compute Diarization Error Rate (DER) Using pyannote.metrics\n",
        "# -------------------------------\n",
        "def compute_der(hyp_segments, ref_segments, collar=0.25):\n",
        "    \"\"\"\n",
        "    Compute Diarization Error Rate (DER) using pyannote.metrics.\n",
        "\n",
        "    Args:\n",
        "        hyp_segments (list of dict): Hypothesis segments (each with 'start', 'end', and 'speaker').\n",
        "        ref_segments (list of dict): Reference segments (each with 'start', 'end', and 'speaker').\n",
        "        collar (float): Forgiveness collar in seconds.\n",
        "\n",
        "    Returns:\n",
        "        float: DER percentage.\n",
        "    \"\"\"\n",
        "    # Prepare segments in the format required by pyannote (list of tuples: (start, end, speaker))\n",
        "    hyp = [(seg[\"start\"], seg[\"end\"], seg[\"speaker\"]) for seg in hyp_segments]\n",
        "    ref = [(seg[\"start\"], seg[\"end\"], seg[\"speaker\"]) for seg in ref_segments]\n",
        "\n",
        "    # Convert to pyannote annotation format\n",
        "    from pyannote.core import Annotation, Segment\n",
        "    ann_ref = Annotation()\n",
        "    for start, end, spk in ref:\n",
        "        ann_ref[Segment(start, end)] = spk\n",
        "    ann_hyp = Annotation()\n",
        "    for start, end, spk in hyp:\n",
        "        ann_hyp[Segment(start, end)] = spk\n",
        "\n",
        "    der_metric = DiarizationErrorRate(collar=collar, skip_overlap=False, ignore_overlap=False)\n",
        "    der = der_metric(ann_ref, ann_hyp)\n",
        "    return der * 100  # as percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_ami_speaker_templates(annotations_dir, audio_file, spk_model, target_sr=TARGET_SR):\n",
        "    \"\"\"\n",
        "    Build speaker enrollment templates from AMI ground truth segments.\n",
        "\n",
        "    This function parses the AMI annotation XML files to obtain ground truth segments,\n",
        "    groups segments by speaker, extracts the corresponding portions of the main AMI audio,\n",
        "    computes speaker embeddings using the pre-trained SpeechBrain model, and averages them\n",
        "    to produce robust templates.\n",
        "\n",
        "    Args:\n",
        "        annotations_dir (str): Directory containing AMI annotation XML files.\n",
        "        audio_file (str): Path to the main AMI audio file.\n",
        "        spk_model: Pre-trained speaker embedding model.\n",
        "        target_sr (int): Target sampling rate.\n",
        "\n",
        "    Returns:\n",
        "        dict: Mapping from AMI speaker ID (e.g., 'ES2008a.sync.3') to an averaged embedding (numpy array).\n",
        "    \"\"\"\n",
        "    # Parse all AMI annotations\n",
        "    ami_segments = parse_all_ami_annotations(annotations_dir)\n",
        "    if not ami_segments:\n",
        "        print(\"No AMI ground truth segments found.\")\n",
        "        return {}\n",
        "\n",
        "    # Load the main AMI audio file\n",
        "    waveform, sr = load_audio(audio_file, target_sr=target_sr)\n",
        "\n",
        "    # Group segments by speaker\n",
        "    speaker_segments = {}\n",
        "    for seg in ami_segments:\n",
        "        speaker = seg.get(\"speaker\", \"Unknown\")\n",
        "        speaker_segments.setdefault(speaker, []).append(seg)\n",
        "\n",
        "    ami_templates = {}\n",
        "    for speaker, segments in speaker_segments.items():\n",
        "        embeddings_list = []\n",
        "        for seg in segments:\n",
        "            start_sample = int(seg[\"start\"] * sr)\n",
        "            end_sample = int(seg[\"end\"] * sr)\n",
        "            seg_waveform = waveform[:, start_sample:end_sample]\n",
        "            # Ensure a minimum segment length by padding if necessary\n",
        "            min_samples = int(1.0 * target_sr)\n",
        "            if seg_waveform.shape[1] < min_samples:\n",
        "                pad_amount = min_samples - seg_waveform.shape[1]\n",
        "                seg_waveform = F.pad(seg_waveform, (0, pad_amount))\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    emb = spk_model.encode_batch(seg_waveform.to(DEVICE)).squeeze().cpu().numpy()\n",
        "                embeddings_list.append(emb)\n",
        "            except Exception as e:\n",
        "                print(f\"Error extracting embedding for AMI segment {seg}: {e}\")\n",
        "        if embeddings_list:\n",
        "            # Average embeddings for a robust template\n",
        "            ami_templates[speaker] = np.mean(embeddings_list, axis=0)\n",
        "    return ami_templates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "VI0RjMpf4Xqg"
      },
      "outputs": [],
      "source": [
        "# -------------------------------\n",
        "# Main Processing Pipeline\n",
        "# -------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Kymdm-w0VE",
        "outputId": "8322c837-e1ce-403b-c101-d4509dee994b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline started at 2025-04-14 16:00:22.641536\n"
          ]
        }
      ],
      "source": [
        "start_time = datetime.now()\n",
        "print(f\"Pipeline started at {start_time}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rJSaX_Uw1yb",
        "outputId": "01a0a9e0-e00b-43e6-c1fc-8f77e86b2c78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading audio file...\n",
            "Audio loaded, duration: 1043.36 seconds, sampling rate: 16000 Hz\n"
          ]
        }
      ],
      "source": [
        "# Load audio file\n",
        "print(\"Loading audio file...\")\n",
        "waveform, sr = load_audio(AMI_AUDIO_PATH, target_sr=TARGET_SR)\n",
        "total_audio_duration = waveform.shape[1] / sr\n",
        "print(f\"Audio loaded, duration: {total_audio_duration:.2f} seconds, sampling rate: {sr} Hz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6TtJ7Sbw4Jh",
        "outputId": "e292d791-46d5-4700-b095-fc3464e06df3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running VAD...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/aryan-kumar/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timestamps seem to be in samples; converting to seconds.\n",
            "Detected 192 speech segments via VAD.\n"
          ]
        }
      ],
      "source": [
        "# Perform Voice Activity Detection (VAD)\n",
        "print(\"Running VAD...\")\n",
        "detected_speech_segments = perform_vad(waveform, sr, vad_threshold=VAD_THRESHOLD, min_speech_duration=MIN_SPEECH_DURATION)\n",
        "print(f\"Detected {len(detected_speech_segments)} speech segments via VAD.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "OvbNuj00w-CX"
      },
      "outputs": [],
      "source": [
        "# Extract audio segments corresponding to VAD output\n",
        "segments_audio = extract_audio_segments(waveform, sr, detected_speech_segments)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkd2FL46w_p7",
        "outputId": "0604bb05-63ad-4956-983a-4ca1117c5547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading speaker embedding model (SpeechBrain ECAPA)...\n",
            "MODEL 'SpeechBrain ECAPA' LOADED SUCCESSFULLY\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained speaker embedding model (SpeechBrain ECAPA-TDNN)\n",
        "print(\"Loading speaker embedding model (SpeechBrain ECAPA)...\")\n",
        "try:\n",
        "    spk_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\", run_opts={\"device\": DEVICE})\n",
        "    print(\"MODEL 'SpeechBrain ECAPA' LOADED SUCCESSFULLY\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading speaker embedding model: {e}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pd3NzhZkxBap",
        "outputId": "b707cf2a-107a-4e59-d674-6a8ec6ea4f84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting speaker embeddings...\n",
            "Extracted embeddings for 192 segments.\n"
          ]
        }
      ],
      "source": [
        "# Extract speaker embeddings for each speech segment\n",
        "print(\"Extracting speaker embeddings...\")\n",
        "embeddings = extract_embeddings(segments_audio, spk_model, sr=sr)\n",
        "print(f\"Extracted embeddings for {embeddings.shape[0]} segments.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTwnGi7vxC1i",
        "outputId": "a4c76e01-908e-4949-d40e-772e78d145dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clustering embeddings for diarization...\n"
          ]
        }
      ],
      "source": [
        "# Cluster embeddings to perform speaker diarization\n",
        "print(\"Clustering embeddings for diarization...\")\n",
        "cluster_labels = cluster_embeddings(embeddings, distance_threshold=DISTANCE_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assigned speaker labels: {'Speaker 61', 'Speaker 45', 'Speaker 189', 'Speaker 3', 'Speaker 125', 'Speaker 40', 'Speaker 128', 'Speaker 105', 'Speaker 31', 'Speaker 141', 'Speaker 64', 'Speaker 132', 'Speaker 183', 'Speaker 37', 'Speaker 30', 'Speaker 67', 'Speaker 88', 'Speaker 187', 'Speaker 69', 'Speaker 131', 'Speaker 170', 'Speaker 68', 'Speaker 53', 'Speaker 168', 'Speaker 136', 'Speaker 16', 'Speaker 74', 'Speaker 71', 'Speaker 164', 'Speaker 171', 'Speaker 138', 'Speaker 39', 'Speaker 6', 'Speaker 66', 'Speaker 50', 'Speaker 78', 'Speaker 188', 'Speaker 43', 'Speaker 140', 'Speaker 111', 'Speaker 13', 'Speaker 112', 'Speaker 77', 'Speaker 79', 'Speaker 29', 'Speaker 104', 'Speaker 147', 'Speaker 12', 'Speaker 107', 'Speaker 133', 'Speaker 99', 'Speaker 87', 'Speaker 19', 'Speaker 26', 'Speaker 33', 'Speaker 192', 'Speaker 143', 'Speaker 23', 'Speaker 95', 'Speaker 173', 'Speaker 5', 'Speaker 25', 'Speaker 91', 'Speaker 113', 'Speaker 49', 'Speaker 182', 'Speaker 96', 'Speaker 186', 'Speaker 175', 'Speaker 180', 'Speaker 117', 'Speaker 142', 'Speaker 185', 'Speaker 72', 'Speaker 157', 'Speaker 94', 'Speaker 101', 'Speaker 21', 'Speaker 176', 'Speaker 114', 'Speaker 119', 'Speaker 52', 'Speaker 59', 'Speaker 121', 'Speaker 8', 'Speaker 190', 'Speaker 155', 'Speaker 181', 'Speaker 32', 'Speaker 73', 'Speaker 82', 'Speaker 110', 'Speaker 129', 'Speaker 84', 'Speaker 36', 'Speaker 55', 'Speaker 20', 'Speaker 2', 'Speaker 162', 'Speaker 144', 'Speaker 161', 'Speaker 130', 'Speaker 127', 'Speaker 44', 'Speaker 57', 'Speaker 174', 'Speaker 108', 'Speaker 153', 'Speaker 169', 'Speaker 80', 'Speaker 75', 'Speaker 118', 'Speaker 89', 'Speaker 151', 'Speaker 35', 'Speaker 184', 'Speaker 56', 'Speaker 150', 'Speaker 122', 'Speaker 58', 'Speaker 47', 'Speaker 46', 'Speaker 165', 'Speaker 83', 'Speaker 76', 'Speaker 179', 'Speaker 134', 'Speaker 166', 'Speaker 65', 'Speaker 27', 'Speaker 7', 'Speaker 18', 'Speaker 98', 'Speaker 115', 'Speaker 167', 'Speaker 135', 'Speaker 159', 'Speaker 191', 'Speaker 11', 'Speaker 172', 'Speaker 10', 'Speaker 160', 'Speaker 156', 'Speaker 22', 'Speaker 152', 'Speaker 100', 'Speaker 60', 'Speaker 4', 'Speaker 124', 'Speaker 137', 'Speaker 28', 'Speaker 158', 'Speaker 178', 'Speaker 90', 'Speaker 139', 'Speaker 1', 'Speaker 149', 'Speaker 102', 'Speaker 24', 'Speaker 103', 'Speaker 145', 'Speaker 51', 'Speaker 109', 'Speaker 14', 'Speaker 86', 'Speaker 70', 'Speaker 97', 'Speaker 54', 'Speaker 93', 'Speaker 48', 'Speaker 123', 'Speaker 163', 'Speaker 154', 'Speaker 62', 'Speaker 106', 'Speaker 42', 'Speaker 9', 'Speaker 146', 'Speaker 15', 'Speaker 126', 'Speaker 92', 'Speaker 148', 'Speaker 81', 'Speaker 17', 'Speaker 177', 'Speaker 85', 'Speaker 116', 'Speaker 120', 'Speaker 38', 'Speaker 41', 'Speaker 34', 'Speaker 63'}\n"
          ]
        }
      ],
      "source": [
        "generic_labels = assign_speaker_labels(cluster_labels)\n",
        "print(f\"Assigned speaker labels: {set(generic_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load enrolled speaker templates (optional)\n",
        "# print(\"Loading enrolled speaker templates...\")\n",
        "# enrolled_templates = load_voxceleb2_templates(ENROLLED_TEMPLATES_DIR, spk_model, target_sr=TARGET_SR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading AMI enrolled speaker templates...\n"
          ]
        }
      ],
      "source": [
        "# Load AMI enrolled speaker templates to reduce domain mismatch for speaker recognition.\n",
        "print(\"Loading AMI enrolled speaker templates...\")\n",
        "enrolled_templates = build_ami_speaker_templates(AMI_ANNOTATIONS_DIR, AMI_AUDIO_PATH, spk_model, target_sr=TARGET_SR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 194 enrolled templates.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Loaded {len(enrolled_templates)} enrolled templates.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Low similarity score (0.394) for cluster 11, assigning generic label.\n",
            "Speaker recognition completed. Recognized IDs: {'ES2008a.sync.79', 'ES2008a.sync.75', 'ES2008a.sync.248', 'ES2008a.sync.19', 'ES2008a.sync.167', 'ES2008a.sync.125', 'Speaker 12', 'ES2008a.sync.143', 'ES2008a.sync.77', 'ES2008a.sync.165', 'ES2008a.sync.278', 'ES2008a.sync.359', 'ES2008a.sync.163', 'ES2008a.sync.270', 'ES2008a.sync.290', 'ES2008a.sync.53', 'ES2008a.sync.91', 'ES2008a.sync.355', 'ES2008a.sync.123', 'ES2008a.sync.351', 'ES2008a.sync.181', 'ES2008a.sync.365', 'ES2008a.sync.129', 'ES2008a.sync.137', 'ES2008a.sync.246', 'ES2008a.sync.345', 'ES2008a.sync.234', 'ES2008a.sync.13', 'ES2008a.sync.383', 'ES2008a.sync.27', 'ES2008a.sync.67', 'ES2008a.sync.131', 'ES2008a.sync.347', 'ES2008a.sync.361', 'ES2008a.sync.101', 'ES2008a.sync.268', 'ES2008a.sync.329', 'ES2008a.sync.11', 'ES2008a.sync.35', 'ES2008a.sync.9', 'ES2008a.sync.57', 'ES2008a.sync.103', 'ES2008a.sync.256', 'ES2008a.sync.221', 'ES2008a.sync.385', 'ES2008a.sync.65', 'ES2008a.sync.260', 'ES2008a.sync.381', 'ES2008a.sync.373', 'ES2008a.sync.23', 'ES2008a.sync.341', 'ES2008a.sync.99', 'ES2008a.sync.280', 'ES2008a.sync.93', 'ES2008a.sync.179', 'ES2008a.sync.349', 'ES2008a.sync.367', 'ES2008a.sync.107', 'ES2008a.sync.258', 'ES2008a.sync.327', 'ES2008a.sync.51', 'ES2008a.sync.7', 'ES2008a.sync.5', 'ES2008a.sync.49', 'ES2008a.sync.15', 'ES2008a.sync.81', 'ES2008a.sync.45', 'ES2008a.sync.37', 'ES2008a.sync.105', 'ES2008a.sync.272', 'ES2008a.sync.209', 'ES2008a.sync.55', 'ES2008a.sync.207', 'ES2008a.sync.121', 'ES2008a.sync.95', 'ES2008a.sync.230'}\n"
          ]
        }
      ],
      "source": [
        "# Perform speaker recognition, if templates are available; otherwise use generic labels\n",
        "if enrolled_templates:\n",
        "    recognized_ids = perform_speaker_recognition(embeddings, cluster_labels, enrolled_templates, recognition_threshold=RECOGNITION_THRESHOLD)\n",
        "    final_speaker_labels = [recognized_ids[label] for label in cluster_labels]\n",
        "    print(\"Speaker recognition completed. Recognized IDs:\", set(recognized_ids.values()))\n",
        "else:\n",
        "    final_speaker_labels = generic_labels\n",
        "    print(\"No enrolled templates found; using generic speaker labels.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQFvwqGxxEuo",
        "outputId": "5cd3e937-f70f-4c78-8375-fab6b9163146"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Whisper ASR model on CPU (MPS not supported for Whisper)...\n"
          ]
        }
      ],
      "source": [
        "# Load Whisper ASR model (adjust model size as needed)\n",
        "ASR_DEVICE = torch.device(\"cpu\")\n",
        "print(\"Loading Whisper ASR model on CPU (MPS not supported for Whisper)...\")\n",
        "try:\n",
        "    asr_model = whisper.load_model(\"medium\", device=ASR_DEVICE)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Whisper ASR model: {e}\")\n",
        "    sys.exit(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "ngiFL-uXxGaX",
        "outputId": "51a5d8cb-eec0-42d4-c234-623c2be69a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running ASR on each speech segment...\n"
          ]
        }
      ],
      "source": [
        "# Transcribe each speech segment with assigned speaker labels\n",
        "print(\"Running ASR on each speech segment...\")\n",
        "transcript_entries = transcribe_segments(waveform, sr, detected_speech_segments, final_speaker_labels, asr_model, language=\"en\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert transcript_entries (tuples) to dicts for merging and evaluation\n",
        "hyp_segments = []\n",
        "for start, end, speaker, transcript in transcript_entries:\n",
        "    hyp_segments.append({\"start\": start, \"end\": end, \"speaker\": speaker, \"transcript\": transcript})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merging adjacent segments with the same speaker...\n"
          ]
        }
      ],
      "source": [
        "# Merge adjacent hypothesis segments with the same speaker if gap is below threshold\n",
        "print(\"Merging adjacent segments with the same speaker...\")\n",
        "merged_hyp_segments = merge_segments(hyp_segments, gap_threshold=GAP_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "final_transcript = [(seg[\"start\"], seg[\"end\"], seg[\"speaker\"], seg.get(\"transcript\", \"\")) for seg in merged_hyp_segments]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(32.706,\n",
              "  36.926,\n",
              "  'ES2008a.sync.5',\n",
              "  \"Okay, good morning everybody. I'm glad you could all come. I'm really excited to start this team.\"),\n",
              " (37.538,\n",
              "  41.374,\n",
              "  'ES2008a.sync.5',\n",
              "  \"I'm just gonna have a little PowerPoint presentation for us for our kickoff meeting.\")]"
            ]
          },
          "execution_count": 94,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "final_transcript[:2]  # Display first 2 entries for verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "YfXj3UotxH6q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving transcript to CSV...\n",
            "Transcript saved to ./Outputs/ES2008a_transcript.csv\n"
          ]
        }
      ],
      "source": [
        "# Save the transcript to a CSV file\n",
        "print(\"Saving transcript to CSV...\")\n",
        "save_transcript_csv(final_transcript, OUTPUT_CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "XDEs3wWaweXK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Aggregating ground truth annotations from all XML files...\n"
          ]
        }
      ],
      "source": [
        "# Evaluation: Aggregate ground truth segments from all AMI annotation XML files\n",
        "print(\"Aggregating ground truth annotations from all XML files...\")\n",
        "reference_segments = parse_all_ami_annotations(AMI_ANNOTATIONS_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "pI53Fr3NwfWG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'start': 27.863, 'end': 28.533, 'speaker': 'ES2008a.sync.3'},\n",
              " {'start': 32.265, 'end': 44.976, 'speaker': 'ES2008a.sync.5'},\n",
              " {'start': 46.112, 'end': 61.813, 'speaker': 'ES2008a.sync.7'},\n",
              " {'start': 63.84, 'end': 99.762, 'speaker': 'ES2008a.sync.9'},\n",
              " {'start': 101.136, 'end': 122.88, 'speaker': 'ES2008a.sync.11'}]"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reference_segments[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "fC9y5vGHxJju"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VAD F1-Score: 0.812\n"
          ]
        }
      ],
      "source": [
        "if reference_segments:\n",
        "    vad_f1 = compute_vad_f1(detected_speech_segments, reference_segments, total_audio_duration)\n",
        "    print(f\"VAD F1-Score: {vad_f1:.3f}\")\n",
        "else:\n",
        "    print(\"No ground truth VAD annotations available for evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyp_segments_for_der = [{\"start\": entry[0], \"end\": entry[1], \"speaker\": entry[2]} for entry in transcript_entries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'start': 32.706, 'end': 36.926, 'speaker': 'ES2008a.sync.5'},\n",
              " {'start': 37.538, 'end': 41.374, 'speaker': 'ES2008a.sync.5'},\n",
              " {'start': 41.986, 'end': 43.934, 'speaker': 'ES2008a.sync.7'},\n",
              " {'start': 46.146, 'end': 54.046, 'speaker': 'ES2008a.sync.7'},\n",
              " {'start': 54.37, 'end': 56.35, 'speaker': 'ES2008a.sync.7'}]"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hyp_segments_for_der[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "i8IVN12S0sNo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Diarization Error Rate (DER): 38.75%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aryan-kumar/.pyenv/versions/speech-diarization-env/lib/python3.11/site-packages/pyannote/metrics/utils.py:200: UserWarning: 'uem' was approximated by the union of 'reference' and 'hypothesis' extents.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Diarization Error Rate (DER) Evaluation\n",
        "# Prepare hypothesis segments from transcript (using start, end, and speaker)\n",
        "if reference_segments:\n",
        "    der_value = compute_der(hyp_segments_for_der, reference_segments, collar=0.25)\n",
        "    print(f\"Diarization Error Rate (DER): {der_value:.2f}%\")\n",
        "else:\n",
        "    print(\"No reference segments available for DER evaluation.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "0sjR9im4xSmR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pipeline completed at 2025-04-14 16:18:04.474946, elapsed time: 0:17:41.833410\n"
          ]
        }
      ],
      "source": [
        "end_time = datetime.now()\n",
        "print(f\"Pipeline completed at {end_time}, elapsed time: {(end_time - start_time)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demo_inference_custom(audio_file, spk_model, asr_model, enrolled_templates=None):\n",
        "    \"\"\"\n",
        "    Run the entire pipeline on a custom audio file and return a merged transcript.\n",
        "\n",
        "    Args:\n",
        "        audio_file (str): Path to the custom audio file.\n",
        "        spk_model: Pretrained speaker embedding model.\n",
        "        asr_model: Pretrained Whisper ASR model.\n",
        "        enrolled_templates (dict, optional): Enrolled speaker templates for recognition.\n",
        "\n",
        "    Returns:\n",
        "        list: Merged transcript segments with 'start', 'end', 'speaker', and 'transcript'.\n",
        "    \"\"\"\n",
        "    # Load custom audio file\n",
        "    waveform, sr = load_audio(audio_file, target_sr=TARGET_SR)\n",
        "    print(f\"Loaded custom audio: duration {waveform.shape[1]/sr:.2f} sec, sampling rate: {sr}\")\n",
        "\n",
        "    # Perform Voice Activity Detection (VAD)\n",
        "    speech_segments = perform_vad(waveform, sr, vad_threshold=VAD_THRESHOLD, min_speech_duration=MIN_SPEECH_DURATION)\n",
        "    print(f\"Detected {len(speech_segments)} speech segments.\")\n",
        "\n",
        "    # Extract audio segments from the detected speech regions\n",
        "    segments_audio = extract_audio_segments(waveform, sr, speech_segments)\n",
        "\n",
        "    # Extract speaker embeddings for each speech segment\n",
        "    embeddings = extract_embeddings(segments_audio, spk_model, sr=sr)\n",
        "\n",
        "    # Cluster the embeddings for diarization\n",
        "    cluster_labels = cluster_embeddings(embeddings, distance_threshold=DISTANCE_THRESHOLD)\n",
        "\n",
        "    # Assign speaker labels (generic or using enrolled templates if available)\n",
        "    speaker_labels = assign_speaker_labels(cluster_labels)\n",
        "    if enrolled_templates:\n",
        "        recognized_ids = perform_speaker_recognition(embeddings, cluster_labels, enrolled_templates, recognition_threshold=RECOGNITION_THRESHOLD)\n",
        "        speaker_labels = [recognized_ids[label] for label in cluster_labels]\n",
        "    print(\"Speaker labels assigned.\")\n",
        "\n",
        "    # Transcribe the segments using Whisper ASR\n",
        "    transcripts = transcribe_segments(waveform, sr, speech_segments, speaker_labels, asr_model, language='en')\n",
        "\n",
        "    # Merge adjacent segments if they belong to the same speaker\n",
        "    merged_segments = merge_segments([{\"start\": s[0], \"end\": s[1], \"speaker\": s[2], \"transcript\": s[3]} for s in transcripts], gap_threshold=GAP_THRESHOLD)\n",
        "    return merged_segments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded custom audio: duration 24.00 sec, sampling rate: 16000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/aryan-kumar/.cache/torch/hub/snakers4_silero-vad_master\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Timestamps seem to be in samples; converting to seconds.\n",
            "Detected 7 speech segments.\n",
            "Low similarity score (0.271) for cluster 0, assigning generic label.\n",
            "Low similarity score (0.234) for cluster 1, assigning generic label.\n",
            "Low similarity score (0.275) for cluster 2, assigning generic label.\n",
            "Low similarity score (0.258) for cluster 3, assigning generic label.\n",
            "Low similarity score (0.162) for cluster 4, assigning generic label.\n",
            "Low similarity score (0.176) for cluster 5, assigning generic label.\n",
            "Low similarity score (0.149) for cluster 6, assigning generic label.\n",
            "Speaker labels assigned.\n",
            "Demo Transcript for Custom Hinglish Speech:\n",
            "[2.18-3.49] Speaker 5: This is Aryan Kumar.\n",
            "[4.90-6.53] Speaker 6: I am speaking from India.\n",
            "[8.26-9.50] Speaker 7: This is Robby Sharma.\n",
            "[10.40-12.09] Speaker 4: I am also speaking from India\n",
            "[13.12-14.24] Speaker 2: I'm in Bangalore.\n",
            "[15.39-19.20] Speaker 3: This is Humana Rorah and I'm also talking from Bangalore.\n",
            "[19.30-23.01] Speaker 1: Nice to meet you all. I am very happy to meet you all.\n"
          ]
        }
      ],
      "source": [
        "# Run demo inference with the custom audio file\n",
        "custom_output = demo_inference_custom(CUSTOM_AUDIO_PATH, spk_model, asr_model, enrolled_templates)\n",
        "print(\"Demo Transcript for Custom Hinglish Speech:\")\n",
        "for seg in custom_output:\n",
        "    print(f\"[{seg['start']:.2f}-{seg['end']:.2f}] {seg['speaker']}: {seg['transcript']}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "speech-diarization-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
